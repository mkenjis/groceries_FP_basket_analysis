
val df = spark.read.format("csv").option("inferSchema","true").option("header","true").load("groceries/Groceries_dataset.csv")

scala> df.printSchema
root
 |-- Member_number: integer (nullable = true)
 |-- Date: string (nullable = true)
 |-- itemDescription: string (nullable = true)
 
scala> df.orderBy("Member_number","Date").show
+-------------+----------+-------------------+
|Member_number|      Date|    itemDescription|
+-------------+----------+-------------------+
|         1000|15-03-2015|semi-finished bread|
|         1000|15-03-2015|             yogurt|
|         1000|15-03-2015|            sausage|
|         1000|15-03-2015|         whole milk|
|         1000|24-06-2014|             pastry|
|         1000|24-06-2014|        salty snack|
|         1000|24-06-2014|         whole milk|
|         1000|24-07-2015|    misc. beverages|
|         1000|24-07-2015|        canned beer|
|         1000|25-11-2015|            sausage|
|         1000|25-11-2015|   hygiene articles|
|         1000|27-05-2015|               soda|
|         1000|27-05-2015| pickled vegetables|
|         1001|02-05-2015|               curd|
|         1001|02-05-2015|        frankfurter|
|         1001|07-02-2014|         whole milk|
|         1001|07-02-2014|         rolls/buns|
|         1001|07-02-2014|            sausage|
|         1001|12-12-2014|         whole milk|
|         1001|12-12-2014|               soda|
+-------------+----------+-------------------+
 
scala> df.groupBy("Member_number","Date").count.show
+-------------+----------+-----+
|Member_number|      Date|count|
+-------------+----------+-----+
|         2747|05-07-2015|    2|
|         4670|11-10-2015|    2|
|         1668|21-08-2015|    2|
|         1843|27-11-2015|    2|
|         2264|12-01-2015|    2|
|         3305|24-06-2015|    2|
|         1239|02-07-2015|    2|
|         2171|28-07-2015|    2|
|         3661|27-11-2015|    3|
|         1340|23-01-2015|    2|
|         1135|24-02-2015|    2|
|         4839|03-06-2015|    2|
|         2643|08-11-2015|    2|
|         3953|13-05-2015|    3|
|         3389|17-08-2015|    3|
|         3998|28-03-2015|    4|
|         4919|12-03-2015|    4|
|         3922|01-04-2015|    4|
|         2589|29-04-2015|    3|
|         2415|08-08-2015|    6|
+-------------+----------+-----+

scala> df.select("itemDescription").distinct.count
res4: Long = 167

val rdd = df.rdd.map( x => x.toSeq.toArray)

val rdd1 = rdd.map( x => (x(0).toString + ":" + x(1).toString,x(2).toString.trim)).distinct

rdd1.take(10)
res21: Array[(String, String)] = Array((1324:10-07-2015,bottled water), (2262:04-10-2015,other vegetables), (4553:13-06-2015,baking powder), (4589:24-01-2014,pork), (3210:23-12-2015,herbs), (1697:21-05-2015,beverages), (3002:13-08-2015,yogurt), (4503:09-08-2015,bottled water), (3628:01-08-2014,female sanitary products), (4042:08-08-2015,white wine))

val prods = rdd1.aggregateByKey(List[Long]())(
  (prods, tran) => prods ::: List(tran),
  (prods1, prods2) => prods1 ::: prods2)

prods.take(10)
res22: Array[(String, List[String])] = Array((1477:14-03-2014,List(packaged fruit/vegetables, beverages)), (4468:12-03-2014,List(bottled water, sliced cheese, onions)), (3584:14-03-2015,List(meat, photo/film)), (3584:17-12-2015,List(whole milk, other vegetables)), (4298:22-03-2014,List(whole milk, dessert, margarine)), (1452:25-11-2015,List(other vegetables, tropical fruit, rice)), (1707:22-12-2014,List(bottled beer, brown bread)), (4326:29-09-2015,List(pork, rolls/buns, root vegetables)), (2899:08-01-2014,List(canned beer, soda, frankfurter)), (3491:06-06-2015,List(root vegetables, fruit/vegetable juice)))

val transactions = prods.map( x => x._2.toSeq.toArray )

transactions.take(10)
res23: Array[Array[String]] = Array(Array(packaged fruit/vegetables, beverages), Array(bottled water, sliced cheese, onions), Array(meat, photo/film), Array(whole milk, other vegetables), Array(whole milk, dessert, margarine), Array(other vegetables, tropical fruit, rice), Array(bottled beer, brown bread), Array(pork, rolls/buns, root vegetables), Array(canned beer, soda, frankfurter), Array(root vegetables, fruit/vegetable juice))

-------------- Not worked using MLlib !!! -------------------------

import org.apache.spark.mllib.fpm.FPGrowth

val fpg = new FPGrowth().setMinSupport(0.2).setNumPartitions(10)

val model = fpg.run(transactions)

model.freqItemsets.collect().foreach { itemset =>
  println(s"${itemset.items.mkString("[", ",", "]")},${itemset.freq}")
}

val minConfidence = 0.8
model.generateAssociationRules(minConfidence).collect().foreach { rule =>
  println(s"${rule.antecedent.mkString("[", ",", "]")}=> " +
    s"${rule.consequent .mkString("[", ",", "]")},${rule.confidence}")
}

--------------- Worked using Spark ML !!! -------------------------

val baskets_ds = transactions.toDF("items")

scala> baskets_ds.show
+--------------------+
|               itens|
+--------------------+
|[packaged fruit/v...|
|[bottled water, s...|
|  [meat, photo/film]|
|[whole milk, othe...|
|[whole milk, dess...|
|[other vegetables...|
|[bottled beer, br...|
|[pork, rolls/buns...|
|[canned beer, sod...|
|[root vegetables,...|
|[other vegetables...|
|[shopping bags, c...|
|[tropical fruit, ...|
|[canned beer, sea...|
|[abrasive cleaner...|
|[UHT-milk, domest...|
|[Instant food pro...|
|  [meat, whole milk]|
|[brown bread, fra...|
|   [root vegetables]|
+--------------------+

import org.apache.spark.ml.fpm.FPGrowth

val fpgrowth = new FPGrowth().setItemsCol("items").setMinSupport(0.001).setMinConfidence(0)

val model = fpgrowth.fit(baskets_ds)

val mostPopularItemInABasket = model.freqItemsets

mostPopularItemInABasket.createOrReplaceTempView("mostPopularItemInABasket")

scala> spark.sql("select * from mostPopularItemInABasket").show
+--------------------+----+
|               items|freq|
+--------------------+----+
|      [cocoa drinks]|  16|
|      [canned fruit]|  21|
|  [specialty cheese]|  72|
|[chocolate marshm...|  60|
|          [pet care]|  85|
|[house keeping pr...|  45|
|       [light bulbs]|  29|
|               [jam]|  34|
|              [beef]| 508|
| [beef, frankfurter]|  15|
|[beef, domestic e...|  17|
|  [beef, rolls/buns]|  24|
|[beef, root veget...|  25|
|[beef, bottled beer]|  16|
| [beef, canned beer]|  15|
|      [beef, yogurt]|  33|
|[beef, fruit/vege...|  16|
|  [beef, newspapers]|  25|
|[beef, bottled wa...|  20|
|[beef, other vege...|  42|
+--------------------+----+

scala> spark.sql("select items, freq from mostPopularItemInABasket where size(items) > 2 order by freq desc limit 20").show
+--------------------+----+
|               items|freq|
+--------------------+----+
|[sausage, yogurt,...|  22|
|[yogurt, rolls/bu...|  20|
|[rolls/buns, othe...|  18|
|[sausage, rolls/b...|  17|
|[yogurt, other ve...|  17|
|[soda, rolls/buns...|  17|
|[soda, other vege...|  17|
|[sausage, soda, w...|  16|
|[soda, rolls/buns...|  15|
+--------------------+----+

